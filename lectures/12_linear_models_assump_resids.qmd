---
title: "297.101 Week 12 Lecture"
subtitle: "Model Assumptions and Residuals"
author: Dr. Nick Knowlton
format: 
  revealjs:
    code-copy: hover
    embed-resources: true
    css: "knowlton.css"
    theme: sky
    slide-number: true
    incremental: true
    smaller: true
    html-math-method:
      method: mathjax
      url: https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js
    highlight-style: breezedark
    highlight-lines: true
    progress: true
    logo: graphics/660491-ls_black_whitoutunz.png
execute:
  output: true
  eval: true
  code-fold: true   # Add this to enable code folding
editor: 
  markdown: 
    wrap: 72

code-tools: true
---

```{r setup, echo=FALSE}
library(knitr)
library(ggplot2); theme_set(theme_bw(base_size=15))
library(patchwork)
library(fontawesome)
opts_chunk$set(dev.args=list(bg='transparent'), comment="", warning=FALSE, echo=FALSE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.dim=c(8,5), out.width="100%", fig.retina=2)
knitr::opts_chunk$set(dev = "svglite")
```

## Learning Outcomes

- Least square estimation.
- Assumptions of the linear model.
- Diagnose departure from assumptions of the linear model with plots of residuals


## Recap

- The general idea: find a (best possible) line to summarize the relationship between two variables, by finding values of the intercept and slope parameters.

- This is the job of the `lm()` function in R.

- We can summarise the results of a `lm()` object with `summary()`.

- We can plot the fitted line onto a the scatter plot of $x$ vs $y$ with `geom_smooth()`.


## Focus

- When is a linear regression model appropriate? 

- How can we find the best intercept and the slope? What criterion does `lm()` use to choose them?

- What is the underlying model for linear regression? (L-I-N-E)

- What assumptions lead to the criterion that `lm()` uses?

# Least Sqaures Estimation

## A mathematical formualtion of the linear model
$$
y=\alpha+\beta x +\varepsilon
$$

- Response variable: $y$ 

- Explanatory/predictor variable: $x$

- Regression coefficients (parameters): $\alpha$ and $\beta$

- Random error (residual): $\varepsilon$

## Minimising the (squared) residuals

- Denote a pair of arbitrary guesses by $\tilde{\alpha}$ and $\tilde{\beta}$. Without divine intervention, these won't exactly equal the true values $\alpha$ and $\beta$. **Why not?**

- For each point $x_i$, guesses $\tilde{\alpha}$ and $\tilde{\beta}$, we have the corresponding estimated value $\hat{y}_i = \tilde\alpha + \tilde\beta x_i$.

- The difference between the real $y_i$ and fitted $\hat{y}_i$ is the **residual** $\hat\varepsilon_i = y_i - \hat{y}_i$.

- A good combination of $\tilde{\alpha}$ and $\tilde{\beta}$ minimizes the residuals -- gets them close to zero. 


## Minimising the variance of residuals


If we have only two pairs of observations, we can easily draw a line between them with residuals zero. 

:::{.fragment}
Given more then two points, what can we do ?
:::

- We need some tools to measure the overall performance of a fitted line in terms of residuals. 


- One way is to *minimise the variance of the residuals*, subject to their mean being 0. 

:::{.fragment .fade-up} 
Why mean 0?
:::

:::{.fragment .fade-up}
With mean 0, the minimization of variance is equivalent to minimization of the sum of squares.
:::

## Least-squares estimation


This is called **least-squares estimation**, as the formula for variance contains a sum of squares
$$
\begin{aligned}
\mathsf{var}(\tilde\varepsilon_i) &= \frac{1}{n}\sum_{i=1}^n (\tilde\varepsilon_i - 0)^2= \frac{1}{n}\sum_{i=1}^n [y_i - (\tilde\alpha + \tilde\beta x_i)]^2
\end{aligned}
$$

. . .

By minimising the **residual variance** $\mathsf{Var}_\mathsf{res} = \frac{1}{n}\sum_{i=1}^n [y_i - (\hat\alpha + \hat\beta x_i)]^2$, we obtain the corresponding values of $\tilde\alpha$ and $\tilde\beta$ which are called **least squares estimates**.

. . .

We often put hats on parameters ( $\hat{\alpha}$, $\hat{\beta}$ ) to denote the least-squares estimates. This helps us to remember that they're sample statistics, differentiating them from the true values $\alpha$ and $\beta$ from the population and the arbitrary guesses $\tilde\alpha$ and $\tilde\beta$. 

. . .

In previous lectures we sometimes used $a$ and $b$ to talk about $\hat{\alpha}$ and $\hat{\beta}$.

. . .

Let's not dwell on the mathematical details. Conveniently, R does all the work for us!


## Estimators vs Estimates


Using a toy example of two points on the x-y plane $(x_1,y_1)$ and $(x_2,y_2)$. 

. . .

We can calculate the residual variance as

$$\mathsf{Var}_\mathsf{res}(\tilde\beta)=[(x_1^2+x_2^2)\tilde\beta^2-2(x_1y_1+x_2y_2)\tilde\beta+(y_1^2+y_2^2)]/2$$

. . .

The minimum of residual variance is attained at the least-squares estimate 
$$\hat\beta=\frac{x_1y_1+x_2y_2}{x_1^2+x_2^2}$$

. . .

This generic form of $\hat\beta$ is called '**the least-squares estimator**'. It is generic, because you can apply it to any set of paired values of $x$ and $y$. How easy is that?!

## Estimators vs estimates

Let's try it out...

- Suppose we have two points $(x_1,y_1)=(1,2)$ and $(x_2,y_2)=(2,3)$.
- The least-squares estimator for $\beta$ is $\hat\beta=\frac{x_1y_1+x_2y_2}{x_1^2+x_2^2}$.
- Substituting the values of $x_1,y_1,x_2,y_2$ into the formula, we get $\hat\beta=\frac{1\cdot2+2\cdot3}{1^2+2^2}=\frac{8}{5}=1.6$.



## Estimators vs estimates


An **estimate** is calculated from the real data but an **estimator** is calculated (derived) from the abstract data. An 'estimator' is a method. An 'estimate' is a result obtained from applying the method.

. . .

If you know the mathematical form of the least-squares estimator, you can easily obtain the least-squares estimates for a particular sample. 

. . .

The summary of `lm()` provides us the least-squares estimates, rather than the estimators. Actually, the least-squares estimators are packaged up in the `lm()` function so we can run it with any data set.

. . .

Statisticians spend a lot of effort to find good estimators and programming them into R packages!


## Estimator as a random variable

Recall that $y_1=\beta x_1+\varepsilon_1$ and $y_2=\beta x_2+\varepsilon_2$. Substituting the right hand side for $y$ in our least-squares estimator $\hat\beta=\frac{x_1y_1+x_2y_2}{x_1^2+x_2^2}$ yields

$$\hat\beta=\frac{x_1(\beta x_1+\varepsilon_1)+x_2(\beta x_2+\varepsilon_2)}{x_1^2+x_2^2}=\beta+\frac{x_1\varepsilon_1+x_2\varepsilon_2}{x_1^2+x_2^2}$$

$\hat\beta$ can be regarded as the sum of the ground truth $\beta$ and the random disturbance $\frac{x_1\varepsilon_1+x_2\varepsilon_2}{x_1^2+x_2^2}$.

. . .

Therefore, $\hat\beta$ is also a random variable! [But what does this mean?]{.fragment} [Why does it vary, and over what?]{.fragment}

:::{.fragment}
The source of the randomness is called '*sampling variation*'. 
:::

:::{.fragment}
If you were to take a different sample from the population, the observed $y$ values would be different. 

- With different $y$ values, you get different residuals $\varepsilon$ (deviations of individual $y$ values from their fitted values $\hat{y}$).

- Thus, the estimates of $\hat\beta$ vary across different sample datasets.
:::



# The Gauss-Markov Assumptions {.center}
![](graphics/lineAssumpWC.webp){.center}


## Assumptions of the linear model: L-I-N-E

**L**inearity

 - $x$ and $y$ are linearly related. Residuals don't depend on $x$. 

**I**ndependence

 - Residuals don't influence each other.

**N**ormality

 - Residuals are distributed normally.

**E**qual variance

 - Residuals have constant variance. The variation doesn't change as we move along the trend.


## A mathematical formualtion of the linear model

Say, we have observed multiple pairs of $y$ and $x$ as $(x_1,y_1),(x_2,y_2),\ldots,(x_n,y_n)$.

$$
y_1=\alpha+\beta x_1 +\varepsilon_1,
$$
$$
y_2=\alpha+\beta x_2 +\varepsilon_2,
$$
$$
\vdots
$$
$$
y_n=\alpha+\beta x_n +\varepsilon_n
$$

## Gauss-Markov Assumptions 

Given a linear model $y_i=\alpha+\beta x_i +\varepsilon_i,i=1,\ldots,n$, all four assumptions can be summarized in the following expression:

$$\varepsilon_i \mathop{\sim}\limits_\mathsf{iid} \mathsf{Normal}(0, \sigma^2)$$

- $\mathsf{mean}[\varepsilon_i]=0$ ensures the **L**inearity.

- $\mathsf{iid}$(*identically and independently distributed*) ensures the **I**ndependence.

- $\mathop{\sim}\mathsf{Normal}$ ensures the **N**ormality. 

- $Var(\varepsilon_i)=\sigma^2$ ensures the **E**qual variance

:::{.fragment}
These four assumptions are called the **the Guass-Markov assumptions**.
:::

## Sampling distribution of our estimators


Under the Guass-Markov assumptions (LINE), we know the distribution of $\hat\alpha$ and $\hat\beta$ across different samples.

. . .

It can be shown that $\mathsf{mean}[\hat\alpha]= \alpha$ and $\mathsf{mean}[\hat\beta]=\beta$. What does this mean?   

. . .

On average, across samples, the least-squares estimators get it right. The distribution of $\hat\alpha$ and $\hat\beta$ are centered on the true parameters $\alpha$ and $\beta$. In other words, they're unbiassed estimators!

. . .

We also know that the standard errors of $\hat\alpha$ and $\hat\beta$ decrease as the sample size $n$ gets larger. The bigger the sample, the closer the estimates get to their true values.

. . .

Knowledge of the distribution of estimates $\hat\alpha$ and $\hat\beta$ can be used to test hypotheses about the true values $\alpha$ and $\beta$.

## Wny do we need assumptions? 


- Without assumptions, there can be no conclusions.


- These four assumptions are critical for deriving the theory of linear model.


- They are even more critical in practice. If the assumptions are not true and we go ahead and assume them anyway, **our statistical inferences can be wrong**.


- Some assumptions are more important than others - the seriousness of violating them varies.



# Residual Diagnostics 




## Checking the assumptions


How do we know if the assumptions are met?

:::{.fragment .fade-up}
We can examine the data and output from the linear model to help us assess whether the assumptions are appropriate for any given case. 
:::

. . .

However, for any real data set and model, you may never know for sure. 

. . .

To check the linear model assumptions, we'll use the standard [diagnostic plots]{.fragment .highlight-red}.



## Checking the assumptions


The **diagnostic plots** can help us assess linearity, normality, and equal variance. 

. . .

We usually can't assess the assumption of independence just by examining the diagnostic plots. We need to consider other information, such as how the data were collected.

. . .

We can also use the diagnostic plots to look for  **outliers** - data points that have unusually large influence on the fit of the model.

. . .

In R, we can use `plot(model)` to get four standard diagnostic plots:

:::{.fragment}

[1. Residuals (versus fitted values) plot]{.fragment .fade-in}

[2. Normal Q-Q plot]{.fragment .fade-in}

[3. Scale-location plot]{.fragment .fade-in}

[4. Residuals versus leverage plot]{.fragment .fade-in}

:::



## Example: Moroccan Donkey data

```{r echo=TRUE, eval=FALSE}
read.csv("https://www.massey.ac.nz/~jcmarsha/227215/data/donkey.csv") %>% 
  lm(Bodywt ~ Heartgirth, data = .) %>%
  plot(., pch=19, col="#00000040")
```



```{r, fig.align='left', fig.width=5, fig.height=4, out.width="65%", echo=FALSE}
donkey <- read.csv("https://www.massey.ac.nz/~jcmarsha/227215/data/donkey.csv")
mod <- lm(Bodywt ~ Heartgirth, data=donkey)
par(mfrow=c(2,2), mar=c(4,2,2,2)) # this splits the plot window into four panels
plot(mod, pch=19, col="#00000040")
```


## Example: Moroccan Donkey data


```{r echo=TRUE, eval=FALSE}
read.csv("https://www.massey.ac.nz/~jcmarsha/227215/data/donkey.csv") %>% 
  lm(Bodywt ~ Heartgirth, data = .) %>%
  plot(., pch=19, col="#00000040")
```



```{r, fig.align='left', fig.width=5, fig.height=4, out.width="65%", echo=FALSE}
par(mfrow=c(2,2), mar=c(4,2,2,2)) # this splits the plot window into four panels
plot(mod, pch=19, which=1, col="#00000040")
```


## Diagnostic plot 1: residuals versus fitted values

:::{.columns}

:::{.column width="60%"}

```{r, echo=FALSE, out.width="100%"}
plot(mod, which=1, pch=19, col="#00000040")
```

:::

:::{.column width="40%"}

:::{.fragment}
We can use this plot to assess:

[1. **linearity** - there should be no curvature, and]{.fragment .fade-up}

[2. **equal variance** - the spread should be constant, with not increase from left to right.]{.fragment .fade-up}

:::

:::

[When either of these fail, a log transformation of $y$ and/or $x$ often helps!]{.fragment .fade-up}

::: 

## Linearity Shiny App

<iframe src="https://shiny.massey.ac.nz/jcmarsha/linearity/" style="border: none" width="100%" height="600px"></iframe>

[Linearity Shiny App](https://shiny.massey.ac.nz/jcmarsha/linearity/)


## Diagnostic plot 2: Normal Q-Q plot

:::{.columns}

:::{.column width="60%"}

```{r, echo=FALSE, out.width="100%"}
plot(mod, which=2, pch=19, col="#00000040")# , xaxt="n")
```

:::

:::{.column width="40%"}

:::{.fragment}

We use this plot to assess **normality** of residuals.

:::
:::{.fragment}

Ideally the points will lie on the straight line.

:::
:::{.fragment}

Some departure from the line, particularly at the ends, is no big deal.

:::
:::{.fragment}

The Central Limit Theorem means this can generally be ignored.
:::

:::
:::


## Diagnostic plot 3: Scale-location plot

:::{.columns}

:::{.column width="60%"}

```{r, echo=FALSE, out.width="100%"}
plot(mod, which=3, pch=19, col="#00000040")

```

:::

:::{.column width="40%"}

:::{.fragment}
We use this plot to assess **equal** or **homogeneity of variance**.

:::
:::{.fragment}

Ideally, points should be flat - not increasing or decreasing.

:::
:::{.fragment}

Residuals versus fit often tells you this just as well, but all residuals are made positive here.

:::
:::
:::

## Diagnostic plot 4: Residuals versus leverage


```{r, echo=FALSE, out.width="100%"}
plot(mod, which=5, pch=19, col="#00000040")
```



We use this plot to look for **influential outliers**.

Points should ideally be inside red bands (Cook's distance) at 0.5.

Points outside Cook's distance of 1 have excessive influence.




## Influential outliers

Points can be outliers in two ways:

1. They can have extreme $x$ values compared to the rest of the data. Such points are said to have high **leverage**.
    
2. They can have extreme $y$ values, given their $x$ value (i.e. a large residual.)

:::{.fragment}

Points have large **influence** if they exhibit both these properties.
:::
:::{.fragment}
**Cook's distance** is a measure of the influence of a data point on the parameter estimates. Cook's distance larger than 1 means the points have large influence on the model fit. Removing these points would change the model.
:::

## Leverage Shiny App

<iframe src="https://shiny.massey.ac.nz/jcmarsha/influence/" style="border: none" width="100%" height="500px"></iframe>

[Shiny Leverage App](https://shiny.massey.ac.nz/jcmarsha/influence/)

---

## Implication of outliers

The challenge of outliers is that they can significantly distort the results, and 'swamp' the contribution of the majority of the data.

. . .

We have seen the outliers in a boxplot and having them often has a significant effect on our mean and standard deviation. Similarly, outliers in our linear model will generally distort the regression coefficient estimates and prediction results. 

. . .

You may choose to remove outliers, but it must be reported and justified in your write up. 

. . .

Remember that ouliers and data errors are different. If I switched the weight of a donkey with a horse, that would be an error, not an outlier. 

:::{.fragment .highlight-blue}
Always check your data!
:::

## Implication of outliers

Be careful though! The outliers may be very informative, perhaps even lead us to new scientific discoveries!

> "Outliers, the data that fall outside the groups, are part of every, or almost, every experiment. An outlier is a data point that is different from the pattern of data. Outliers often contribute to negative results. Scatterplots and boxplots can help detect stray data points. Technical errors can be recognized by going over the experimental records, data collection and analysis. Outliers can be removed from analyses due to human mistake, unbiased application of a priori defined inclusion and exclusion criteria, or statistical tests."

::: aside
Marina E. Emborg, “Reframing the perception of outliers and negative data in translational research”,*Brain Res Bull*\ https://doi.org/10.1016%2Fj.brainresbull.2022.11.020
:::

## Donkeys: Residuals *vs* fitted values

:::{.columns}

:::{.column width="50%"}

```{r, echo=FALSE, out.width="100%"}
plot(mod, which=1, pch=19, col="#00000040")
```

:::

:::{.column width="50%"} 

:::{.fragment}

* Linearity doesn't hold. There is a curve.

* Equal variance doesn't hold. The residuals are increasingly spread out to the right of the plot.

:::

:::

:::


## Tweaking data to meet assumptions


:::{.columns}

:::{.column width="50%"}
#### Original scale

```{r,fig.align='center', fig.width=8, fig.height=5}
ggplot(donkey, aes(x=Heartgirth, y=Bodywt)) + geom_point()
```
:::

:::{.column width="50%"}
#### Log scale

```{r,fig.align='center', fig.width=8, fig.height=5}
ggplot(donkey, aes(x=log(Heartgirth), y=log(Bodywt))) + geom_point()
```
:::

:::

## Tweaking data to meet assumptions

:::{.columns}

:::{.column width="50%"}
#### Original scale

```{r}
mod1 <- lm(Bodywt ~ Heartgirth, data=donkey)
plot(mod1, which=1, pch=19, col="#00000040")
```
:::

:::{.column width="50%"}
#### Log scale

```{r}
mod2 <- lm(log(Bodywt) ~ log(Heartgirth), data=donkey)
plot(mod2, which=1, pch=19, col="#00000040")
```
:::

:::

. . .

Linearity and equal variance now hold.


## Summary

- Assumptions for linear model

- Least squares estimation

- Estimator, estimate, and sampling distribution

- Residual diagnostics 



  
