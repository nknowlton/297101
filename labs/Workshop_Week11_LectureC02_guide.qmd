---
title: "Workshop Week 10"
subtitle: "Regression Part I: Inference and Prediction"
author:
  - name: "Nick Knowlton"
date: "Spring 2025"
date-format: "[Spring 2025]"
format: 
  html:
    toc: true
    code-fold: false
    code-download: true
embed-resources: true
resources:
  - regression/cancer_data.csv.gz
  
output-dir: _html
---

```{r include=FALSE}
#| warning: false
library(tidyverse) # For Tidy programming
library(broom) # To clean up the model outputs
library(skimr) # To have access to the skim function
library(GGally)
library(scales)
```

## Instructions

-   This is a hands-on workshop where you will work on exercises to understand inference and prediction in linear regression models.
-   Follow the instructions and complete the tasks in the provided code cells.
-   Note that this file is a Quarto document. It's like a Rmd document but it uses the Quarto language. You shouldn't see any difference in the code or text sections, so input your code chunks as usual and hit "render" not "knit" to create the output `html`.

## Learning Objectives

-   Perform inference on linear regression models.
-   Understand key summary statistics (e.g., coefficients, confidence intervals).
-   Make predictions based on linear models and interpret the results.

## Excercise 0: Get Familiar with your data

### Cancer Data Across America üó∫Ô∏è

Before we dive into our regression analysis, let's talk about the fascinating dataset we'll be exploring! 

We're working with **lung cancer data from across the United States** - think of it as taking the pulse of America's health, county by county. This dataset combines information from multiple sources:

- **Cancer incidence and mortality rates** from the National Cancer Institute's SEER program
- **Socioeconomic data** from the U.S. Census Bureau's American Community Survey  
- **Health insurance coverage** statistics

Each row in our dataset represents a different **U.S. county**, identified by something called a **FIPS code**. FIPS (Federal Information Processing Standards) codes are like zip codes for counties - a unique 5-digit identifier that helps us keep track of America's 3,000+ counties. The first two digits represent the state, and the last three represent the specific county within that state.

Why lung cancer data? Well, lung cancer remains one of the leading causes of cancer death in the U.S., and there are interesting patterns in how it varies across different communities. Some areas have much higher rates than others - and we want to understand why! Is it related to poverty levels? Access to healthcare? Population density? That's what we're here to find out! üïµÔ∏è‚Äç‚ôÄÔ∏è

*Note: This dataset was preprocessed following the excellent tutorial by [Nick Rippner](https://data.world/nrippner/cancer-analysis-hackathon-challenge), which walks through the complete data gathering and cleaning process.*

You can [download the dataset (CSV.GZ)](regression/cancer_data.csv.gz).

```{r}
library(readr)
cancer <- read_csv("regression/cancer_data.csv.gz", show_col_types = FALSE)



```{r Import_data}

cancer<- read_csv(file="./regression/cancer_data.csv.gz", show_col_types = F) # Don't forget the gz ending

```
## Take a look at the data.

Now that the data is loaded the first thing to do is *glance* at the data and run some quick scatter plots to see if we see any relationships in the data. We are looking for a few things at this stage.

- What scale are the data on? It is common to receive data a bit **dirty** and you need to understand the units presented.

```{r DataPeek}

skim(cancer)


```

**Answer**: We see that medium income for black, native american, asian and hispanic are <= 80% complete. That might lead to some issues when we create models with the data as we need complete cases (no missing values) to estimate coefficients. So, lets remove those additional med income values. 

```{r clean_income}

cancer <- cancer |>
  select(-starts_with("med_income_"))

```

Now we will do a quick graphical look at our data using a modified draftsman plot.First you will need to remove the non numeric variables from the data frame. 

> **HINT: States, Areas, and FIPS are strings. Some of the *rate variables were imported as strings**

```{r draftsman}
#| warning: false

ggpair_data <- cancer |> 
  select(-c(state,area_name,FIPS,incidence_rate,avg_ann_incidence))

p <- ggpairs(
  ggpair_data,
  progress = FALSE,
  diag = list(continuous = "densityDiag"),
  lower = list(continuous = wrap("points", size = 12)),
  upper = list(continuous = wrap("cor", size = 3))
) +
  theme_minimal(base_size = 11)
p
```

That seems a mess! Too many variables. Let's grab a potential outcome variable and a small subset of predictors and try again. 

>**Hint:** Let's keep poverty, med income and mortality and incidence rates.

```{r ggpairs_try2}
#| warning: false

data_try2 <- cancer |>
  select(all_poverty,m_poverty,f_poverty,med_income,Mortality_Rate_num,Incidence_Rate_num,popestimate2015)


p <- ggpairs(
 data_try2,
  progress = FALSE,
  diag = list(continuous = "densityDiag"),
  lower = list(continuous = wrap("points", size = 12)),
  upper = list(continuous = wrap("cor", size = 3))
) +
  theme_minimal(base_size = 11)
p

```

If we didn't spot it earlier, the poverty data looks like counts! More people in an area will likely give more people in poverty leading to potentially meaningless associations. As these values get very large, we will take the logarithm now, but later we will will correct these counts to rate per 100k people as incidence and mortality is reported to clean up the relationships observed.   

```{r cancer_workshop}

cancer_clean <- data_try2 |>
  mutate(all_poverty_log = log10(all_poverty))

```

## Exercise 1: Inference Based on a Linear Model

### Step 1: Fit a Linear Model

1.  Fit a linear model of `Incidence_Rate_num` based on `all_poverty` and save the result in `lm.poverty`. Then, view the summary of the model:

**Answer**:

```{r}

lm.poverty <- lm(Mortality_Rate_num ~ all_poverty_log, data = cancer_clean)
summary(lm.poverty)
```

> **Reflection**: Make sure to explore the different parts of the output in your notebook. Write down observations, focusing on what each part of the output represents. We will dive deeper into the most important sections next.

### Step 2: Understanding Coefficients

2.  In the model summary, focus on the `Coefficients:` section. Here are key points to note:
    -   **Intercept and Slope**: These are our estimated coefficients. The `Estimate` column shows their values.
    -   **Point Estimate**: These are only estimates from our sample. Ask yourself, could these values differ if we had more data?
    -   **Standard Error**: This indicates the variability of these estimates. We only have a sample, so our estimate may not perfectly match the true population value.

> **Task**: Construct a 95% confidence interval for the slope (`all_poverty`) and the intercept. Use the summary output to calculate the interval manually.

#### Hint:

For the slope: $$
-2.8592 \pm 2 \times 0.47
$$ The 2 comes from a normal approximation (use the `qnorm()` function if needed). Calculate the confidence interval and interpret it.

**Answer**:

```{r}
conf.level <- 0.95
qnorm(1 - (1 - conf.level) / 2)
```

#### Slope Confidence Interval:
Using the formula $-2.8592 \pm 2 \times 0.47$, we get:

```{r}

c(-2.8592-2*0.47,-2.8592+2*0.47)

```


> **üí° Think About It**: A confidence interval gives us a range of plausible values for the true parameter. If we repeated this study 100 times, about 95 of those confidence intervals would contain the true slope value. It's like saying "we're pretty confident the true effect is somewhere in this range."

#### Intercept Confidence Interval:
For the intercept, the confidence interval is:
```{r}

c(63.93-2*1.7855,63.93+2*1.7855)

```


> **Question**: Based on your confidence interval, is -3.5 a plausible value for the true coefficient of `all_poverty`?

**Answer**: Yes, the `all_poverty` slope is within -3.5, which is plausible.

### Step 3: P-Values and Hypothesis Testing

3.  The next key column in the summary is `Pr(>|t|)`, which gives the p-values. This is used to test whether the coefficients are significantly different from zero:
    -   **t-value**: This represents how many standard deviations the estimate is from zero.
    -   **p-value**: A low p-value (e.g., \<0.05) suggests that the coefficient is significantly different from zero, implying a real relationship between `Mortality_Rate_num` and `all_poverty`.

> **Task**: Use the `t value` to calculate the p-value manually using this code:

**Answer**

```{r}
t.value <- 2
2 * (1 - pnorm(t.value))
```

```{r TvaluetoPvalue}

t.value.slope <- -6.085
2 * pnorm(-abs(t.value.slope))


```
The result is approximately 0, which matches the summary output.


> **Reflection**: Think about the significance codes (`'***'`, `'*'`, etc.) in the summary. What do these codes tell you about the strength of evidence against the null hypothesis?

### Step 4: Residual Variance and R-squared

4.  Finally, we calculate the ratio of the variance of residuals to the variance of `Mortality_Rate_num`. This ratio helps us understand how much of the variability in `Mortality_Rate_num` is explained by `all_poverty`.

**Hint**: Use the `augment()` function from the `broom` package to calculate the residuals.

**Answer**:

```{r}

augment(lm.poverty) |> summarise(ratio.var = var(.resid) / var(Mortality_Rate_num),
                                 R_squared = 1 - ratio.var)

```

> **Question**: Compare this ratio with the `Multiple R-squared:` value in the summary. What does `R-squared` tell us about the relationship between `Mortality_Rate` and `all_poverty`?

**Answer**: The `Multiple R-squared:` value is about 0.013, meaning 1.3% of the variability in `Mortality_Rate` is explained by the `all_poverty` variable. This is very low and although significant statistically, it likely to not be a helpful model.  

> **üìä What R-squared Really Means**: Think of R-squared as the percentage of "mystery" in your data that your model has solved. An R-squared of 0.74 means your model explains 74% of why counties have different mortality rates - that's pretty good! The remaining 26% is due to factors we haven't measured or random variation.

### Step 5: Rerun the model with normalized poverty rate

Earlier I mentioned that the poverty data is count based and the mortality rate is per 100k people. Let's try that now.

> **ü§î Why Normalize?**: Raw counts can be misleading! A county with 1,000 people in poverty might sound worse than one with 500, but what if the first county has 100,000 people total (1% poverty rate) and the second has 1,000 people total (50% poverty rate)? Normalizing helps us compare apples to apples.

```{r PopAdjustment}

cancer_clean <- cancer_clean |>
  mutate(pov_norm = all_poverty / popestimate2015 * 10**5)
```

**Hint**: Use the normalized data to plot poverty vs mortality.

**Answer**

```{r PovNormPlot}
### Now we will plot all poverty vs pov_norm to see how they relate

p <- ggplot(cancer_clean, aes(x = all_poverty, y = pov_norm, color = popestimate2015)) +
  geom_point(alpha = 0.6, size = 1) +
   scale_color_viridis_c(
    name   = "Population",
    trans  = "log10",                        # keep log scaling
    breaks = c(1e3, 1e4, 1e5, 1e6),          # 1000, 10000, 100000, 1000000
    labels = label_number(big.mark = ",")    # format with commas
  ) +
  labs(
    title = "All poverty (count) vs normalized poverty rate (per 100k)",
    x = "All in poverty (count)",
    y = "Poverty per 100k"
  ) +
  theme_minimal(base_size = 12)

print(p)



```
We see that there are many small areas (FIPS) where poverty is very high and the largest poverty count (1,800,265) corresponds to a normalized poverty value in about the middle of the distribution. 


### Step 5: Rerun the model with normalized poverty rate

What has changed in the model output besides the coefficients themselves?
>**Hint: Are the terms still significant? Does this model explain more variablility? Etc**

**Answer**:

```{r PovNormModel}

lm.poverty.norm <- lm(Mortality_Rate_num ~ pov_norm, data = cancer_clean)
summary(lm.poverty.norm)

```
This model explains 14% more variability in mortality by simply normalizing the poverty from a count to a proportion per 100k. This is something you should always look out for when given data. Are the units compatible?

### Step 6: Expanding the Analysis

5.  Now, repeat the analysis for one additional models: `lm.med_income`. Fit this models and compare their summaries as you did for `lm.all_poverty` and `lm.poverty.norm`.

**Answer**:

```{r}

lm.med_income <- lm(Mortality_Rate_num ~ med_income, data = cancer_clean)
summary(lm.med_income)

```

`lm.med_income` has an R-squared of approximately 0.2, this suggests that medium income in an area explains more about mortality rates from lung cancer than the number of people in poverty.

------------------------------------------------------------------------

## Exercise 2: Prediction Based on a Linear Model

**Objective**: Predict deaths based on population poverty rates and understand the role of confidence and prediction intervals.

### Step 1: Predict Deaths

1.  Suppose you have a poverty rate of 75% in a newly annexed American region. Use the model to predict mortality. **Hint: 75% poverty is 100k * 0.75**

> **Task**: Calculate the predicted Mortality_Rate_num manually using the coefficients from the model summary.

**Answer**:

```{r ManualCalcs}

38.35 + .0009130 * 75000

```


### Step 2: Using `predict()`

2.  Use R to automate the prediction process. Create a new dataset and predict the sales:

**Answer**

```{r}

new_data <- data.frame(pov_norm = 75000)
predict(lm.poverty.norm, new_data)


```

> **Self check**: Did you get a value around 106 deaths per 100,000 people?

```{r Poorest}

# Create extrapolated point at 75% poverty (75,000 per 100k)
extrapolated <- tibble(
  area_name = "Extrapolated (75% Poverty)",
  pov_norm = 75000,
  Mortality_Rate_num = 106
)

# Combine for plotting
compare_df <- cancer_clean |> 
  select(pov_norm, Mortality_Rate_num) |> 
  drop_na()

# Jefferson County data (hard-coded since state info isn't in dataset)
jefferson <- tibble(
  area_name = "Jefferson County, MS",
  pov_norm = 46490,               # 46.5% poverty = 46,490 per 100k
  Mortality_Rate_num = 59.7       # observed death rate per 100k
)

# Add to your existing plot
ggplot(compare_df, aes(x = pov_norm, y = Mortality_Rate_num)) +
  geom_point(alpha = 0.4, color = "gray60") +
  geom_point(data = jefferson, aes(x = pov_norm, y = Mortality_Rate_num),
             color = "blue", size = 3) +
  geom_text(data = jefferson, aes(x = pov_norm, y = Mortality_Rate_num,
             label = "Jefferson Co., MS (46.5%, 59.7/100k)"),
             vjust = -1, color = "blue", size = 3.5) +
  labs(
    title = "Highest Observed Poverty and Mortality in Data",
    subtitle = "Jefferson County, Mississippi (hard-coded reference)",
    x = "Poverty per 100,000 People",
    y = "Lung Cancer Mortality (per 100,000)"
  ) +
  theme_minimal(base_size = 12)


```

> **‚ö†Ô∏è Reality Check**: Our model predicts over 106 deaths per 100,000 people for 75% poverty rate. This seems unrealistic because:
> The most impoverished county in the dataset, Jefferson County, Mississippi, has a poverty rate of 46.5% (46,490 per 100,000) and a lung cancer mortality rate of 59.7 deaths per 100,000. When we extrapolate our model to a hypothetical 75% poverty rate, it predicts 106 deaths per 100,000‚Äîalmost double Jefferson County‚Äôs mortality rate and well above the data‚Äôs upper bound. This demonstrates that the prediction is outside the observed data range and not epidemiologically plausible. However, there are some counties with mortality rates above 100/100k, but these have <30% mortality potentially indicating that other factors besides poverty are driving these rates.
> This is why checking your predictions against reality is crucial in data science.




### Step 3: Confidence vs. Prediction Intervals

3.  Explore the difference between confidence intervals (for the mean prediction) and prediction intervals (for an individual prediction). Use R to compute both:

**Answer**

```{r}

predict(lm.poverty.norm, new_data, interval = "confidence")
predict(lm.poverty.norm, new_data, interval = "prediction")

```

The confidence interval is tighter because it estimates the mean (average) response, while the prediction interval is wider as it accounts for individual variability. 

> **Question**: How do the confidence and prediction intervals differ? Which one is wider, and why?

### Step 4: Varying Confidence Levels

4.  Calculate predictions for different confidence levels (90%, 95%, and 99%):

**Answer**: The confidence intervals widen as the confidence level increases. A 99% confidence interval is the widest because it accounts for greater uncertainty.
```{r}

predict(lm.poverty.norm, new_data, interval = "confidence", level = 0.90)
predict(lm.poverty.norm, new_data, interval = "confidence", level = 0.95)
predict(lm.poverty.norm, new_data, interval = "confidence", level = 0.99)

```

> **Task**: Compare the intervals at different confidence levels. What happens as the confidence level increases?

### Step 5: Predicting with No Poverty

5.  You introduce a UBI cutting the all_poverty rate to zero. Predict the mortality under this scenario:

```{r}

new_data0 <- data.frame(pov_norm = 0)
predict(lm.poverty.norm, new_data0, interval = "prediction")

```
The predicted deaths are 38/100k.


> **Reflection**: Does this prediction seem realistic? Why or why not?

**Hint: Try a histogram**


**Answer** :The predicted deaths are still 38/100k. To tell if this value is reasonable, we need to place it in context. Let's use a quick histogram of all mortality rates and see where this falls.

```{r hist}

ggplot(cancer_clean, aes(x = Mortality_Rate_num)) +
  geom_histogram(binwidth = 5, fill = "skyblue", color = "white", boundary = 0) +
  geom_vline(xintercept = 38, color = "red", linetype = "dashed", linewidth = 1) +
  labs(
    title = "Distribution of Mortality Rates across Counties",
    x = "Deaths per 100,000",
    y = "Count of Counties"
  ) +
  theme_minimal(base_size = 12)
```

The value seems very reasonable in this context as there are many low poverty areas upon which the prediction is based.

------------------------------------------------------------------------
## üéØ Key Takeaways

From our analysis, we learned that:

1. **Data cleaning matters**: We had to handle missing values, normalize by population, and deal with data quality issues
2. **Relationships exist**: Counties with higher poverty rates tend to have higher lung cancer mortality rates
3. **Models have limits**: Our predictions break down when we extrapolate beyond the data range
4. **Context is crucial**: Statistical significance doesn't always mean practical significance

## Learning Summary

By completing these exercises, you should be familiar with: 

  * Point estimation, confidence intervals, and hypothesis testing in linear models. 
  * How to make predictions using linear models, including understanding the uncertainty around these predictions.
  * The importance of data cleaning and context in regression analysis.
  * The limitations of models and the dangers of extrapolation.
  